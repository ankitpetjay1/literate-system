# Bulding Data Lakes

### **Module Overview**

1. What are Data Lakes?

2. Data Storage and ETL Options in GC

3. Build Data Lake using Cloud Storage

4. Secure Cloud Storage

5. Cloud SQL as a Relational Data Lake

---

<br>

## **What are Data Lakes?**

_Generally described as a place where you can store your data of all scales for processing and analytics._

Typically used to drive Data Analytics, DS & ML workloads or Batch and streaming data pipelines.

Data Lakes accept all types of data.

Data Lakes are portable. On-Prem and In Cloud

<br>

### **Where does Data Lake fit in Data Engg ecosystem?**

1. Data Sources: Start with systems that are sources of your data.

2. Data sinks: Retrieving and storing your data.

   - Central data lake repository: Takes data of whatever volume, variety of fotmats and velocity.
   - Data warehouse: After the data is off the source systems and inside the environment, processing and cleanup is required to transform into useful format. It then ends in Data Warehouse.

3. Data Pipelines (Batch and Streaming): Everything involved in Cleanup and Processing of data. Responsible for transformations and processing on your data at scale.

4. High-level orchestration workflow: An abstraction layer above your pipelines. Often the components require coordination in between at a regular interval or event-driven cadence. Orchestration workflow kicks off the pipeline when new raw data is available from source.

**Examples**

In Civil Engineering, you have been tasked to build a skyscraper.

Before starting you need to ensure you have all the raw materials.

1. Act of bringing steel, concreete, water, the wood, sand, glass from whatever sources is analogous to data coming from source systems and into your lake.

2. You need to cut the wood, metal, measure and format the glass. The job of a worker is to take the materials and transform them. The end result, cut glass, and shaped metal, this is the format of data stored in your warehouse. We are ready to build the building, or the data is ready to add value to your business.

3. The actual building is the new insight or ML model etc.

4. On a construction site is the manager or supervisor that directs what work needs to be done and any dependencies.

   - As soon as metal or glass arrives, send them to this site for cutting and shaping, and alert another team for building. In Data Engineering this is the orchestration layer.

   - Everytime a new CSV drops in cloud storage bucket, pass it to data pipelines for processing. After that stream it into Data Warehouse. Post that notify the ML model that new data is available for training and direct to start new model version.

> Cloud Storage is one of the options for Data Lake. We can also have BigQuery as both a Data Lake and a Data Warehouse.

<br>

### **Difference between Data Lake and Data Warehouse**

<br>

**Data Lake:**

1. A data lake is where you capture every aspect of your business operations.

2. To capture every aspect data is stored in raw format. The same format it is produced by your application. So you may have a log file stored as is in a data lake.

3. As you want to store all, you tend to store them as object blobs or files.

**Data Warehouse:**

1. To do something useful with the data.

2. You might store the data in warehouse after the schema is defined and the use case is identified.

3. You might take the raw data that exists in data lake transform it, organize it and process it, clean it up, and then store it in a data warehouse.

4. The need for getting data in Data Warehouse is maybe you need to generate charts, reports, dashboards, and so on quickly.

5. The schema is consistent, which makes the data organized and conducive for querying and analysis.

<br>

### **Data Storage and ETL Option in GC**

Options for building a data lake are storage solutions in GC.

- Cloud Storage - All Data
- Cloud SQL - Relational
- Cloud Spanner - Relational
- FireStore - NoSQL
- Cloud BigTable - NoSQL

Choosing right storage depends on use case but we will focus on Cloud Storage and Cloud SQL.

**Path of your data depends on:**

1. Where your data is now?
2. How big is your data?
3. Where it has to go? Ending point also called data sync.
4. How much transformation and processing your data needs?

<br>

**Q. Should you complete processing data before loading into Data Lake or afterwards or before it gets shipped off somewhere else.**

The method that you use to load the data into the cloud depends on **how much transformation is needed** from the raw data.

1. Extract and Load (EL)

   - You have data in the format that is readily ingested in the cloud product. Eg: Avro format. You can store the data to BigQuery because BigQuery can directly load this format.

   - This is the scenario for EL where data can be loaded as is. Typically in scenarios where the source and the target have the same schema.

2. Extract Load Transform (ELT)

   - When the data loaded into the cloud product is not in the final format you want it in.

   - You want to clean it up or maybe transform the data in some way.

   - Eg: Extract from On-Prem, load into the cloud product and then do transformation.

   - This is the scenario when the transformation needed is not very high and would not reduce the amount of data you have.

   - ELT allows raw data to be loaded directly into the target and transformed there. In BigQuery you can use SQL to transform data and write a new table.

3. Extract Transform and Load (ETL)

   - This is the case when you want to extract the data, apply a bunch of processing to it and then load it into the cloud product.

   - Transformation greatly reduces the data size.

   - By transforming data before loading it into the cloud you will be able to greatly reduce the network bandwidth.

   - The data might be transformed in data pipeline like data flow before being loaded in BigQuery.

### **Building Data Lake using Cloud Storage**

Why Cloud Storage popular option to serve as Data Lake?

1. Persistent: Data in cloud storage persists beyong lifetime of VMs or clusters.
2. Inexpensive in compute. You can even cache the results of previous computation in cloud storage. Or save the state of machine in cloud storage and shut it down.
3. Object Store: It stores and retrieves binary objects without regard to what data is contained in objects. Can make objects look like files so you can copy objects in and out of it.
4. Durable: Data stored will be present there forever.
5. Availability: You can share the data globally and encrypt it as well.
6. High throughput: Low data latency.

**How does Cloud Storage Work**

<br>

Two main identities **Buckets** and **Objects**

1. Buckets are containers for objects.
2. Objects exist only inside buckets.

Buckets are identified in single global unique namespace. The name is unique across globe.

When a bucket is created it is associated with single region or multiple regions. Choosing the region where data will be processed reduces latency.

Network egress charges are further reduced if you use cloud services in the same region to process data.

Cloud storage replicates stored objects and monitors replicas. It a copy is corrupted or lost it replaces the objects with fresh ones.

For a single region bucket, the objects are replicated across zones.

For multi region bucket, the objects are replicated across regions.

Multiple requestors may be retrieving the object at the same time from different replicas.

Finally the objects are stored as metadata. Cloud Storage uses them for access control, compression, encryption and life cycle management.Eg: Object can be set to be deleted auto after a period of time, this feature uses metadata to determine when to delete the object.

<br>

**Storage Classes**

Classes are based on how often the data is accessed.

1. Standard storage:

   - Frequently accessed data
   - Stored for brief periods of time.
   - Dual region gives optimized performance.
   - Good for serving websites content, streaming videos, executing interactive workloads or serving data supporting mobile and gaming applications.

2. Nearline Storage:

   - Low cost highly durable storage service for storing infrequently accessed data.
   - Good for storing basis on slightly lower availability and access. 30 day min storage duration.
   - Ideal for data that need to be read or modify on average once per month.
   - Appropriate for backup and archiving.

3. Coldline Storage:

   - Very low cost highly durable storage for storing infrequently accessed data.
   - Better choice than standard storage or nearline storage with slightly lower availability.
   - 90 Day minimum storage duration and higher costs for data access.
   - Ideal for data that needs to be read and modified one in a quarter.

4. Archive Storage

   - Lowest cost highly durable storage service.
   - Data archiving, online backup and disaster recovery
   - Higher cost for data access.
   - 365 days min storage duration.
   - Data that you want to access once a year.

Single API,
Millisecond data access latency.

Object lifecycle management. Uses policy to auto move data to lower cost storage classes as it's access infrequently.

Cloud Storage uses bucket name and object name to simulate a file system. Bucket name is the first name in the URI, forward slash is appended to it, and then concatenated with object name. Object name allows the forward slash character as valid character in the name. It works like a file path even though it is a single name. Eg: `declass/de/modules/02/script.sh`.
declass is the bucket name and the rest is the file name.

To move the data let's say to a directory name 03 in modules, you will have to search all objects that have names 02 in the right position and then change each object name and rename them using 03. This would bring same result as moving file between directories.

To access cloud storage using file access method:
`gs://declass/de/modules/02/script.sh`

To access using web use, [storage.cloud.google.com](storage.cloud.google.com)  
Site uses TLS HTTP to protect data.

Cloud Object management features:

1. Set retention policy on all objects in bucket.
2. Versioning
3. Lifecycle Management: Auto move object that haven't been accessed for 30 days to nearline, and for 90 days to coldline.

<br>

### **Securing Cloud Storage**

1. IAM Policy:
   - Is the standard
   - Set at bucket level
   - Applies uniform access rules to all object within a bucket.
   - Bucket roles (Reader, Writerm Owner)
   - Providesability to create or change access control lists.
   - Project roles allows ability to create and delete buckets and to set IAM policy. (Viewer, Editor and Owner)
2. Access control lists.
   - Can be applied at bucket level or individual objects.
   - More fine grained access control.
   - When creating bucket you have option to disable ACL and only using IAM.
   - Currently enabled by default.

<br>

**Google Managed Encryption Keys GMEK**

All data is encrypted at rest and in transit, no way to turn off the encryption.

Two levels of encryption.

1. Data is encrypted using encryption key.
2. Then the data encryption key is itself encrypted using key encryption key KEK.

   - KEK's are automatically rotated on schedule.
   - Current KEK is stored in cloud KMS Cloud Key Management Service.

   - You can manage KEK youself called **CMEK** Customer Managed Encryption KEK.
   - Supply your own ecryption to eliminate KMS called Customer Supplied Encryption Keys.

3. Client Side Encryption: Encryption applied before data is loaded.
   - Encryptions will still be performed over this object.
   - They have no knowledge of encryptions already present.

**Other Features**

Data logging for bucket retention, objects.

- For audit purpose you can put hold on data and all the data pending for delete will be put on hold.
- Lock can be done on bucket. No changes or deletions can be made.
- Retention policy continus to remain in affect.

Proper object tagging in metadata, causes cloud storage to decompress the file as it is being served. Leads to faster uploads and lower storage costs.

Can set up requester pays for egress traffic to another region. So you pay only for data storage.

Signed URL to anonymously share an object in cloud storage. URL can be made to expire after a period of time.

<br>

### **Store data types**

You don't want to use Cloud Storage for transactional workloads. Cloud storage latency not good enough to support transactional data.

Tools for transactional workloads: Cloud SQL and Firestore.

Also not to use CS for analytics on structured data. Compute parsing data will cost a lot.

Tools for analytics: Cloud BigTable or BigQuery

**Transactional vs Analytics**

Transactional

1. Transactional requires fast inserts and updates.
2. Want to maintain snapshot a current state of the system.
3. Queries tend to be simple and affect few records.

Analytical

- Analytical workload tends to read the entire data set.
- Data might come from transaction processing system but is often consolidated from many OLTP systems.

Data Engineers build pipelines to populate OLAP systems from OLTP systems.

Limit to size of data that you can load to BigQuery. Ideal to load the data to Cloud Storage first. Cmd `gsutil -m cp ...`  
Providing `-m` to gsutil to copy data.

**Relational Databases**

Cloud SQL is default choice but if you require globally distributed data then use Cloud Spanner. If your database needs update from geographically seperated regions.

Cloud Spanner is also useful if your database is too big to fit into single Cloud SQL instance.

BigQuery is default choice for analytical workloads. however if you need low latency and high throughput inserts use Cloud BigTable.

<br>

### **Cloud SQL**

Default choice for Online Transaction Processing Workloads.

Fully managed RDB

Supports MySQL, PostgreSQL, Microsoft SQL Server.

Google basically provides the compute engine which has MySQL installed. Google manages the instance by performing backups, security updates, and other necessary updates.

Can set up failover replica 99.95% SLA.

Cloud SQL can be used with App Engine using standard driver like connector J for Java or MySQL DB for Python.

Compute Engine can be authorized to access Cloud SQL instances using an external IP address. Cloud SQL instances can be configured with same zone as your VM.

Cloud SQL can be used with applications like SQL Workbench, Toad and external applications using standard MySQL drivers.

**Advantages**

1.  Google Security: Encrypted on Google's network, databases, etc. Every Cloud SQL instance includes a network firewall allowing you to control network access to your database instance by granting access.

2.  Managed backups, easy to restore from backup, perform a point in time recovery to a specific state of an instance. Cloud SQL retains up to 7 backups for each instance, included in cost of instance.

3.  Vertical scaling (read and write): Increase your machine size up to 64 processor cores and 100+ GB RAM.

4.  Horizontal Scaling (read): Read replicas.

5.  Automatic replication: 3 read replicas supported:
    1.  Cloud SQL instaces replicate from a primary Cloud SQL instance.
    2.  Cloud SQL instance replicating from external primary instance.
    3.  External MySQL instance replicating from a cloud SQL primary instance

> For Horizontal Read/Write scaling consider Cloud Spanner.

**Cloud SQL Replication**

1. Failover replica can be configured in different zone within same region.

2. Cloud SQL data is replicated across zones within a region for durability.

3. In event of data center outage, a Cloud SQL instance will automatically become available in another zone.

4. All changes made to data on the primary are replicated to failover.

5. And Cloud SQL auto fails over to the replica.

6. Failover can also be initiated manually.

Failover replica is charged as a seperate instance.

Primary fails over to your failover replica, any existing connections to the instance are closed. Apps can connect using same connection string or IP address. No need to update app after failover.

Replica becomes the primary after failover and a new failover replica is created in another zone in the same region.

If your Cloud SQL instances are located near resources such as Compute Engine Instance, you can relocate Cloud SQL instance back to it's original zone when zone becomes available. Or no need to relocate instance after failover.

You can also use failover replica as read replica to offload read operation from primary.

<br>

**Fully Managed vs Serverless**

Fully managed means that the service runs on hardware that you can control. SSH into cloud SQL instance. Google helps manage the instance by automating backups and sending out failover instances.

Serverless is like an API you are calling. You pay for using the product but don't manage the hardware. BigQuery, Pub/Sub, Dataflow are serverless.

Think of Cloud Storage as serverless as well. You never interact with disks.

> You can build a pipeline of well designed componenets all of which are serverless.

Dataproc on the other hand is fully managed. It helps you run Spark or Hadoop workloads without worrying about setup.

<br>

### **Demo:**

1. Create a Cloud SQL instance which can hold multiple databases.

   -

2. Create a Cloud SQL database

3. Import text data into Cloud SQL

4. Check the data for integrity.
