# Build Data Warehouse

### **Module Overview**

1. What makes a Modern Data Warehouse?

2. Intro to BigQuery

3. Getting started with BigQuery

4. Load Data into BigQuery

5. Schemas

6. Schema Design

7. Nested and Repeated Fields

8. Optimize tables in Data Warehouse with Partitioning and Clustering

---

<br>

### **What makes a modern Data Warehouse?**

An enterprise Data warehouse should consolidate data from many sources.

A DW imposes a schema while data lake is just raw data.

A Data Warehouse make it available for querying and data processing.

An analyst needs to know the schema of the data to use a Data Warehouse however does not need to write code to read and parse the data.

Another reason to consolidate data besides standardizing the format of data is making sure the queried results are meaningful; clean, accurate and consistent.

The purpose of DW is not to store data as it is of Data Lake.

It's important to ensure that the queries made to DW are quick.

**What makes it modern?**

- Single DW that can scale from Gigabytes to Petabytes of data.

- Serverless and no-ops, not based on clusters that need to be managed or indexes that need to be fine tuned.

- Supports rich visualization and reporting.

- Modern DW should be able to integrate with ecosystem of processing tools for building ETL pieplines.
- Up to the minute data. Able to stream data not batch updates.
- Support Machine Learning without moving data out of the warehouse.
- Possible to impose security like data exfiltration constraints. Share data with collaborators.

<br>

### **BigQuery**

- Able to scale form gigabytes to petabytes seamlessly.

- Serverless and able to perform Ad-hoc queries and no-ops.

  > BigQuery is similar to the cost of Cloud Storage while providing storage and querying. No need to worry about archiving off older data to save on storage.

- Features like GIS and ML built in.

- Provides capability for real-time data analysis.

- All security while able to share datasets and queries.

- Supports standard SQL queries and is compatible with ANSI SQL 2011.

> Serverless.
>
> Updating a table expiration at the time of table creation.
>
> BigQuery does not run any vacuum processes that runs at various intervals to reshuffle and sort data blocks to recover space. This is because BigQuery storage engine automatically manages and optimizes how data is stored and replicated.
>
> BigQuery does not use indexes on tables. You don't need to rebuild these.
>
> This takes away the worry of managing common database management tasks.

<br>

**BigQuery is Fast**

1. BigQuery tables are column oriented compared to traditional RDBM tables which are row oriented.

2. BigQuery is OLAP system and meant for analytics. Tables are immutable and optimized for reading and appending data. Not optimized for updating.

3. BigQuery leverages the fact that most queries have few columns so it only reads the columns required for query.

4. Row oriented are efficient for making updates to data containing fields. For OLTP systems, row oriented system are necessary as they have frequent updates.

5. Analytics is slow on row oriented tables because they have to read all the fields in a row.

**Implementation**

- BigQuery is implemented as a Storage Engine and an Analytical engine.

- BigQuery data is physically stored on Google's distributed file system called Colossus.

- No need to provision resources before using BigQuery unlike Cloud SQL and other RDBMS systems. BigQuery allocates storage and query resources dynamically based on usage patterns.

- Storage resources are allocated as you consume them and deallocated as you remove data or drop tables.

- Query resources are allocated based on Query type and complexity. Each query comprises of slots which are units of computation. By default all BigQuery customers have access to 2000 slots of query operations.

  > Under the hood analytics throughput is measured in BigQuery slots.
  >
  > A BigQuery slot is a unit of computational capacity required to execute sequel queries.
  >
  > A slot is a combination of CPU, Memory, and networking resources. Each slot may have different specifications.
  >
  > BigQuery automatically calculates how many slots are required at each stage depending on size and complexity.

- You can reserve fixed number of slots for your project. Pricing model for reserved slots is available for customers that want more predictable pricing. However reserving 10,000 slots when having 15,000 slots of consumption can result in each query executing more slowly.

<br>

### **Get started with BigQuery**

- BigQuery organizes data tables into units called **datasets**.

- Each dataset is scoped to your Project.

- When you want to use a table stored in BigQuery service you use the construct _project.dataset.table_

- Projects can isolate datasets according to business needs like billing and access control.

- Different datasets can isolate tables from different analytical domains.

- Data is stored in seperate tables based on logical schema considerations.

- If the query is being done on a public dataset belonging to a different project, the query charges are billed to your project and not to the public data project.

- In order to run a query in a project you need Identity Access Management (IAM) permission to submit a job. You can set data access through **Access control**.

- Access control is done through IAM and can be set at dataset, table, view or column level. In order to query data in a table or view you need atleast read permissions on the table or view.

- Similar to cloud storage, BigQuery datasets can be regional or multi regional. Regional replicates across multiple zones in the region. Multi-Regional replicates across multiple regions.

- Every table has a schema which you can enter manually to the Cloud Console or by supplying a JSON file.

- Similar to Cloud Storage, data is encrypted at rest and over the wire using Google Managed Encryption Keys (GMEK). You can also use Customer Managed Encryption Keys (CMEK) or Cusomer Supplied Encryption Keys (CSEK)

- Authentication is through IAM so it is possible to use gmail or Google Workspace accounts.

- Access control is through IAM roles and involves giving permissions at the Dataset, Tables, View, Column. If you give read or write permissions at the dataset level you give permissions across all the tables within that dataset.

- Admin activities and system events are all logged. Example to system events is table expiration which you can set while creating the table. Logs are also present for every access given to dataset under your project.

- Predefined roles are available for controlling access to resources. Custom IAM roles can be created consisting of your defined set of permissions and then assign those roles to users or groups.

- Marketing, Sales and HR all access same tables but their level of access differ. With access control you can achieve row level access to datasets, tables or views OR by defining authorized views on row-level permissions.

- The access policy acts as a filter to hide or display certain rows of data depending on user or group is in allowed list.

> Authorised user with IAM roles like BigQuery Admin or BigQuery owner can create row-level access policies on BigQuery table.

- You can create policies using SQL in BigQuery.

  ![Access Policy query example](./images/%231.3_access-policy.jpg)

- Giving a view access involves creating an authorised view. This allows you to share query results with users and groups without giving them access to underlying source data. You can also use the views to restrict the columns field the users are able to query.

- Traditionally onboarding new data analysts involves significant lead time, but using Google Cloud you can greatly accelerate an analyst's time to productivity. To onboard an analyst on Google Cloud:

  1. You grant them access to relevant projects.
  2. Analysts can perform tasks in the Cloud Console according to the role you grant them such as viewing metadata, previewing data executing and saving and sharing queries.

- In addition to access controls at the table or column level you can use views. Eg: You can create a dataset B where you store all your views that come from tables in dataset A. Now by providing users access to dataset B we are creating authorised view which is subset of original data.

  > A view must be in the same region as the dataset.
  >
  > You cannot export data from a view.
  >
  > A view appears similar to a table and you can query a view just like a table.
  >
  > We can also create materialized views which keep the view up-to-date with the contents of the source table.

<br>

### **Load Data into BigQuery**

<br>

The method to load data depends on how much transformation is needed.

**EL** is used when source and target have same schema and data is stored as is.

**ELT** is used when raw data is loaded directly into the target and transformed there.

**ETL** is used when transformation happens in an intermediate service before it is loaded into target.

A load job creates a destination table if one doesn't already exist.

You can specify the schema explicitly by passing the schema as an argument to the load job.

Ongoing load jobs can append to the same table using the same procedure as the initial load but do not require the schema to be passed with each job.

If csv files contain headers that must be ignored after the intial load and table creation you can use _skip_leading_rows_ flag to ignore the row.

BigQuery sets limits on number and size of load jobs that you can perform per project and per table.

In addition, BigQuery sets limits on the sizes of individual load files and records.

You can set up cloud functions to automate the process to listen to a Cloud Storage event and launch a BigQuery load job.

BigQuery can import data stored in JSON format as long as it is new line delimitted.

Other formats include _parquet_, _ORC_.

Can also directly import Firestore and data store export files.

Another way to import data is using the API. API is mainly used either from dataproc or dataflow.

BigQuery data transfer service provides connectors and pre-built BigQuery load jobs that perform the transformations necessary to load report data from various services directly into BigQuery.

You can automate execution of queries based on a schedule or event and cache the results for later consumption. These queries can run on recurring basis.

Query string and destination table can be parameterized allowing you to organize query results by data and time.

BigQuery even maintains a 7 day history of changes against your tables by allowing you to query a point in time snapshot of your data. Easily revert changes without having to request a recovery from backups.

You can only recover a deleted table only if another table with the same ID in the dataset has not been created. You cannot recover a deleted table if it is being streamed to. Chances are that the streaming pipeline would have already created an empty table and started pushing rows into it. Also be careful using Create or Replace table as this makes table irrecoverable.

### **Demo**

Loading data into BigQuery
