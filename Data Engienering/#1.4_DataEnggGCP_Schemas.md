# Explore schemas

**Exploring Schmeas**

Demo: [Practice using INFORMATION_SCHEMA and TABLES to explore metadata](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/data-engineering/demos/information_schema.md)

Reading: [GoogleDocs](https://cloud.google.com/bigquery/docs/information-schema-tables#advanced_example)

<br>

### **Schema Design**

Data might exist in many forms. Normalizing data means turning it into relational system.

This stores the data efficiently and makes query processing a clear and direct task.

**Normalizing** is useful for saving space. It usually happens when a schema is designed for a database.

**Denormalizing** is the strategy of allowing duplicate values for a column in a table to gain processing performance. Data is repeated rather than being relational.

This takes more storage but queries become efficient as queries can be processed in parallel using columnar processing.

Denormalizing data enables BigQuery to more efficiently distribute processing among slots resulting in more parallel processing and better query performance.

You usually denormalize data before loading it in BigQuery.

There are cases where denormalizing is bad for performance. Eg: If you want to group by column with a one-to-many relationship. This process requires sending the data over servers for shuffling. Shuffling is slow.

To imporve this situtation BigQuery supports nested and repeated data.

**Nested & Repeated Data**

So instead of denormailized data we have denormalize with nested and repeated data.

![Nested and repeated data](./images/%231.4_denormalized-nested.jpg "Nested and Repeated data")

Nesting can be understood as a form of repeated field. In the above snapshot we have _order.ID_ as repeated field. It preserves the relational quality of original data and schema while enabling columnar and parallel processing of repeated nested fields.

Turning relation into nesting and repeating field improves BigQuery performance.

<br>

### **Nested & Repeated Fields**

**Gojek**

13 PB data per month from queries to support business decisions.

New order. Eg: Ride

> Stored in orders table
>
> Each order has single pick up location and drop off destination.
>
> For single order you may have one or many event timestamps like: ride_ordered,ride_confirmed, ride_on_route, drop_off_complete, etc.

To store all this information, we can store one fact in one place. Typical normalized data method.

Or we can go denormalized route and save all levels of granularity in single big table where orderID is repeated for each event that happens on that order. Repeating happens for storing granular details like an array of event timestamps for that order. Faster for querying but with drawbacks.

JOINs are costly for normalized tables. As RDBMS as record based so they have to open each record entirely. Then pull out JOIN key where match exists. If you want to join say 10 tables then this option is expensive.

Alternative is to prejoin all your tables in one massive table makes reading data faster.

![Denormalized repeated timestamps](./images/%231.4_denormalized-repeated-timestamps.jpg)

Above snapshot stores rows for 4 unique orders.

The grey space between the rows is because event.status and event.time is at a deeper level of granularity. That is there are multiple repeated values for these events per each order. The data type to handle this nested data is an **array**.

Notice event.status and event.time columns being used to store _event_ granularly. These are what we call **structs**. Other structs in table include pickup and destination. Structs are SQL data types.

> Structs can be thought of as pre-joined tables within a table.
>
> Instead of having different table for having event, pickup and destination you simple nest them within main table.

In summary we can go more deep in a single field and have it be more granular by using array data type. And you can have really wide schemas by using structs which allow you to have multiple fields for same or different data types within them.

Major benefit is data is prejoined so it is much faster to query.

[How to work with Arrays and Structs in Google BigQuery](https://medium.com/google-cloud/how-to-work-with-array-and-structs-in-bigquery-9c0a2ea584a6#:~:text=An%20Array%20is%20a%20list,then%20it%20is%20an%20Array.&text=A%20Struct%2C%20on%20the%20other,need%20to%20use%20'dot'.)

Struct and array datatypes in SQL can be absolutely independent of each other. The benefit of using them together is that arrays allow a given field to go deep into granularity and structs allow you to organize all those useful fields into logical containers instead of separate tables. Structs are a type of record when looking at a schema and arrays are of mode repeated.

What to do with your existing star schema snowflake and third normal form data. BigQuery also works well with those schema types.

Use arrays and structs when your data naturally arrives in that format and you'll benefit immediately from optimal performance. For the other schema types, bring them directly to BigQuery.

<br>

**Demo**

[Querying a Bitcoin dataset in BigQuery with nested and repeated columns
](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/data-engineering/demos/nested.md)

<br>

### **Partitioning & Clustering**

In a table partitioned by a DATE or TIMESTAMP column, each partition contains a single day of data. When the data is stored, BigQuery ensures that all the data in a block belongs to a single partition.

This requires BigQuery to maintain more metadata than a non partitioned table. As the number of partitions increases the amount of metadata overhead increases.

One of the ways you can optimize the tables in your data warehouse is to reduce the cost and amount of data read by partitioning your tables yourself.

For example, assume we have partitioned table for dates between 01-03 and 01-04. BigQuery will then change its internal storage, so the data is stored in separate shards. Big query ensures that all the data in a block belongs to a single partition.

Now when you run a query with a where clause that looks for dates between 01-03 and 01-04, BigQuery will have to read only two fifths of the full data set. This can lead to dramatic cost and time savings.

You enable partitioning during the table creation process. This slide shows how to migrate an existing table to an ingestion time partitioned table. Using a destination table, it will cost you one table scan.

As new records are added to the table, they will be put into the right partition.

BigQuery will then create new date based partitions automatically with no need for additional maintenance. In addition, you can specify an expiration time for data in the partitions.

Partitioning can be set by ingestion time on a TIMESTAMP, DATE or DATETIME column or based on a range of an integer column. Here we are partitioning customer_ID in the range 0 to a 100 in increments of 10.

Make sure that the partition field is isolated on the left side because that's the only way BigQuery can quickly discard unnecessary partitions.

**Clustering** can improve the performance of certain types of queries, such as queries that use filter clauses and those that aggregate data.

When data is written to a clustered table by a query or load job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage.

When you submit a query containing a clause that filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data.

Similarly, when you submit a query that aggregates data based on the values in the clustering columns, performance is improved because the sorted blocks co-locate rows with similar values.

You set up clustering at table creation time. The columns you specify in the cluster are used to co-locate related data.

When you cluster a table using multiple columns, the order of columns you specify is important. The order of the specified columns determines the sort order of the data.

Over time, as more and more operations modify a table, the degree to which the data is sorted begins to weaken, and the table becomes only partially sorted.

In a partially sorted table queries that use the clustering columns may need to scan more blocks compared to a table that is fully sorted.

BigQuery now periodically does auto re-clustering for you so you don't need to worry about your cluster's getting out of date as you get new data. Automatic re-clustering is absolutely free and automatically happens in the background.

BigQuery supports clustering for both partition and non partitioned tables.

When you use clustering and partitioning together, the data can be partitioned by a DATE, DATETIME or TIMESTAMP column and then clustered on a different set of columns.

In this case, data in each partition is clustered based on the values of the clustering columns.

Keep in mind if you don't have partitioned columns and you want the benefits of clustering, you can create a fake_date column of type date and have all the values be null.
