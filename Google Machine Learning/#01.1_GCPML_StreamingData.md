# Data Engineering for Streaming Data

**Course Overview**

How to build a real time data solution with Google Cloud Products and services.

1. Ingest streaming data with **Pub/Sub**
2. Process data with **Dataflow**
3. Visualize the results with **Data Studio** and **Looker**

<br>

**Topics Overview**

1. BigData challenges today
2. Message-Oriented Architecture
3. Design streaming pipelines with Apache Beam and implement using Dataflow
4. Visualize data with Looker and Data Studio

Reading Material: [Click here](./Reading%20List/01.1_StreamingData.pdf)

---

<br>

### **Intro**

**Streaming Data vs Batch Processing**

1. Batch Processing: When processing and analysis happens on set of stored data either weekly or monthly basis. Eg: Payroll and Billing system.
2. Streaming Data: Is a flow of data records generated by various data sources. Processing of streaming data happens as the data flows through a system. Helps in analysis and reporting of events as they happen in near realtime. Eg: Fraud detection or intrusion detection.

Modern data processing has progressed from legacy batch processing to working with real time data streams.

### **BigData Challenges**

Data Engineers are responsible for building scalable and reliable pipelines.

4 Major challenges are faced by Data Scientists and Data Engineers today. These are also known as 4 V's:

1. Variety
2. Volume
3. Velocity
4. Veracity

**Variety**

Data could come in from different sources and in different formats. Eg: Number, image, audio.

**Volume**

This added with size of the data being recieved which can vary from GB's to PBs.

Can our infrastructure and pipeline code scale with these or will grind to a hault.

**Velocity**

Data needs to be processed near real time.

You will also need to process data that arrives late, or data with bad data in the message.

All needs to be transformed mid flight as it is being streamed into a data warehouse.

**Veracity**

Refers to Data Quality where the data might face challenges with consistency or uncertainities.

These are typical challenges when designing pipelines.

<br>

### **Message Oriented Architecture**

One of the earlier stages in data pipeline is Data Ingestion which is where large amounts of data is received.

This data might be incomming from millions of points that are happening asynchronously. Eg: Data from IOT's that send data from sensors.

1. Data maybe streamed from different methods and devices.
2. Difficult to distribute event messages to right subscribers.
3. Data arriving quickly and at high volumes.
4. Ensuring services are working as expected.

Google offers **Pub/Sub** to distribute message oritented architecture at scale.

1. Pub/Sub is used to publish messages to subscribers.
2. Pub/Sub can receive messages from variety of device streams like Gaming events, IOTs and Video streams.
3. Ensures at-least-once delivery of received messages to subscribed applications.
4. Does not require provisioning. APIs are open.
5. End to end encryption.

<br>

**End to end Pub/Sub Architecture**

1. Upstream data comes from devices all over the globe. Ingested into Pub/Sub.

2. Pub/Sub reads/stores and broadcasts to any subscribers of this data topic as soon as new messages are available.

   > As a subscriber of Pub/Sub **Dataflow** can ingest and transform messages in an elastic streaming pipeline.
   >
   > It can then output results into an analytics data warehouse like **BigQuery**.
   >
   > You can connect a data visualization tool like **Looker** or **Data Studio**
   >
   > Or you can connect results of a pipeline to an AI or ML tool such as Vertex AI to explore the data to uncover business insights or help with predictions.

3. Pub/Sub has a central element which is the topic.

   - Topic is like a radio antenna. Weather music is playing or turned off, antenna is always there.
   - If music is being broadcast to different frequency that nobody is listening to, stream of music still exists.
   - Similarly a publisher can send data to a topic that has no subscriber to receive it.
   - Or a subscriber can be waiting for data from a topic that isn't getting any data sent to it.

4. In a fully operational data pipeline a publisher can be sending data to a topic that an application is subscribed to.

5. There can be 0 1 or more publishers or there can be 0 1 or more subscribers. Both are decoupled and work without affecting others.

6. Pub/Sub is a good solutions for architectures that have many sources and sinks.

7. You can even publish event from one topic to another.

<br>

### **Designing streaming pipelines with Apache Beam**

After the messages have been captured from input sources you need to pipe the data into Data Warehouse for analysis.

This is where **Dataflow** comes into picture.

1.  Dataflow creates a pipeline that can process both streaming data and batch data. Process refers to _Extract, Transform_ and _Load_ data aka _ETL_.

    > Data Engineers are responsible for coding the pipeline design and implementing/serving pipeline at scale.

2.  Questions to consider during pipeline design phase:

    - Pipeline code compatible with both batch and streaming data?
    - Will pipeline SDK have transformations that are applied mid-flight like aggregations and windowing?
    - Will pipeline SDK be able to handle late data?
    - Templates?

    > Popular solution for pipeline design is **Apache Beam**

3.  **Apache Beam** is an open source, unified programming model to define and execute data processing pipelines, including ETL, batch and stream processing.
    - _Unified_ as it uses single programming model for both batch and streaming data.
    - _Portable_ which means it can work on multiple execution environments like **Dataflow** and **Apache Spark**.
    - _Extensible_, you can write your own connectors and transformation for sources and sinks.
    - It also provides pipeline templates so you don't need to build a pipeline from nothing. Supported lang: Java, Python or Go.
    - Apache SDK packages all software development tools in one installable package.
    - Apache beam creates a model representation from your code > This model is portable across many runners > Runners pass off your model for execution on different possible engines like dataflow.

### Implementing streaming pipelines on Cloud Dataflow

We learnt earlier that **Apache Beam** can be used to create data processing pipelines.

The next step is to identify an execution engine to implement those pipelines.

1. Questions when choosing pipeline engine:

   - Maitenance overhead?
   - Infrastrucutre reliability?
   - Pipeline scaling?
   - Pipeline monitoring?
   - Is pipeline locked to a specific service provider?

2. **Dataflow** is a fully managed service for executing Apache Beam pipelines within Google Cloud.

   - Handles much of the complexity related to infrastructure setup related to build and maintenance.
   - Auto scale infrastructure.

3. Serverless and NoOps.

   - NoOps as it does not require managment from operations team. This is because _maintenance, monitoring_, and _scaling_ are automated
   - Serverless is when Google manages infrastructure on behalf of the user. Google manages _Resource provisioning, Performance tuning_ and _Pipeline reliability_.

4. Dataflow helps organizations spend more time in analysing data and less time provisioning resources.

5. Tasks performed by Dataflow when a job is received:

   - Optimizes a pipeline model's execution graph to remove any inefficiencies.
   - It schedules out distributed work to new workers and scales as needed.
   - Auto heals any worker faults.
   - Worker rebalancing.
   - Outputs data to produce result.
     > BigQuery is one of the options where it can output data to.

6. Experienced Java or Python developers are benefitted by using Dataflow templates. Template categories:
   - Streaming (Processing real time data)
     - Pub/Sub to BigQuery
     - Pub/Sub to Cloud Storage
     - Datastream to BigQuery
     - Pub/Sub to MongoDB
   - Batch
     - BigQuery to Cloud Storage
     - Bigtable to Cloud Storage
     - Cloud Storage to BigQuery
     - Cloud Spanner to Cloud Storage.
   - Utility
     - Bulk compression fo cloud Storage files.
     - Firestore bulk delete.
     - File format conversion.

### **Visualization with Looker and DataStudio**

**Looker**

Success of data pipeline depends on the usefulness of the insights generated from data.

After the data is in BigQuery, lot of effort and skills are still required to uncover insights.

Google provides **Looker** and **Data Studio** to uncover insights from data.

1. Looker supports **BigQuery** as well as 60 different types of SQL database products referred to as dialects.

2. Allows developers to define a semantic modeling layer on top of databases using Looker modeling language or LookML.

   - LookML defines logic and permissions independent from a specific database or a SQL language.
   - Looker is 100% web based.
   - Looker API can be used to embed reports in other applications.

<br>

**Data Studio**

Data Studio is integrated into **BigQuery**, making data visualization possible with few clicks.

Does not require support from an administrator to establish data connection.

Data Studio dashboards can be built steps:

- Choose a template
- Link the dashboard to a data source
- Explore dashboard
